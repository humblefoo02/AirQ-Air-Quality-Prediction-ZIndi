{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7515f98f-e55c-400a-9f2b-2efdc6c79281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amanc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 72 is smaller than n_iter=100. Running 72 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\users\\amanc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 81 is smaller than n_iter=100. Running 81 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 64\n",
      "[LightGBM] [Info] Number of data points in the train set: 8071, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 24.639296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amanc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 27 is smaller than n_iter=100. Running 27 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\users\\amanc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 3 is smaller than n_iter=100. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\users\\amanc\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 3 is smaller than n_iter=100. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 64\n",
      "[LightGBM] [Info] Number of data points in the train set: 8071, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 24.639296\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 64\n",
      "[LightGBM] [Info] Number of data points in the train set: 8071, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 24.639296\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 64\n",
      "[LightGBM] [Info] Number of data points in the train set: 6456, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 24.119842\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 64\n",
      "[LightGBM] [Info] Number of data points in the train set: 6457, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 24.155891\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 64\n",
      "[LightGBM] [Info] Number of data points in the train set: 6457, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 24.370391\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 64\n",
      "[LightGBM] [Info] Number of data points in the train set: 6457, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 25.269886\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 64\n",
      "[LightGBM] [Info] Number of data points in the train set: 6457, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 25.280389\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.svm import SVR\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# Load and preprocess data\n",
    "train_data = pd.read_csv('Train.csv')\n",
    "test_data = pd.read_csv('Test.csv')\n",
    "\n",
    "for data in [train_data, test_data]:\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data['year'] = data['date'].dt.year\n",
    "    data['month'] = data['date'].dt.month\n",
    "    data['day'] = data['date'].dt.day\n",
    "\n",
    "drop_columns = ['id', 'site_id', 'date']\n",
    "categorical_features = ['city', 'country']\n",
    "numerical_features = ['year', 'month', 'day']\n",
    "\n",
    "for data in [train_data, test_data]:\n",
    "    for col in data.select_dtypes(include=np.number).columns:\n",
    "        if data[col].isnull().any():\n",
    "            data[col].fillna(data[col].median(), inplace=True)\n",
    "\n",
    "X_train = train_data.drop(columns=drop_columns + ['pm2_5'])\n",
    "y_train = train_data['pm2_5']\n",
    "X_test = test_data.drop(columns=drop_columns)\n",
    "ids_test = test_data['id']\n",
    "\n",
    "# Preprocessing pipeline\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "numerical_transformer = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('num', numerical_transformer, numerical_features)\n",
    "    ])\n",
    "\n",
    "# Define models\n",
    "svr = SVR()\n",
    "lgbm = LGBMRegressor(random_state=42)\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "ridge = Ridge()\n",
    "lasso = Lasso()\n",
    "\n",
    "# Create pipelines\n",
    "svr_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('svr', svr)])\n",
    "lgbm_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('lgbm', lgbm)])\n",
    "xgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('xgb', xgb)])\n",
    "ridge_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('ridge', ridge)])\n",
    "lasso_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('lasso', lasso)])\n",
    "\n",
    "# Hyperparameter tuning\n",
    "svr_param_grid = {\n",
    "    'svr__kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "    'svr__C': [0.1, 1, 10, 100],\n",
    "    'svr__gamma': ['scale', 'auto'],\n",
    "    'svr__epsilon': [0.01, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "lgbm_param_grid = {\n",
    "    'lgbm__num_leaves': [31, 63, 127],\n",
    "    'lgbm__max_depth': [5, 10, 15],\n",
    "    'lgbm__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'lgbm__n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'xgb__max_depth': [3, 5, 7],\n",
    "    'xgb__n_estimators': [100, 200, 300],\n",
    "    'xgb__learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "ridge_param_grid = {\n",
    "    'ridge__alpha': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "lasso_param_grid = {\n",
    "    'lasso__alpha': [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "# Random search with cross-validation\n",
    "svr_random_search = RandomizedSearchCV(svr_pipeline, svr_param_grid, n_iter=100,\n",
    "                                       scoring='neg_root_mean_squared_error', cv=5, n_jobs=-1, random_state=42)\n",
    "\n",
    "lgbm_random_search = RandomizedSearchCV(lgbm_pipeline, lgbm_param_grid, n_iter=100,\n",
    "                                        scoring='neg_root_mean_squared_error', cv=5, n_jobs=-1, random_state=42)\n",
    "\n",
    "xgb_random_search = RandomizedSearchCV(xgb_pipeline, xgb_param_grid, n_iter=100,\n",
    "                                       scoring='neg_root_mean_squared_error', cv=5, n_jobs=-1, random_state=42)\n",
    "\n",
    "ridge_random_search = RandomizedSearchCV(ridge_pipeline, ridge_param_grid, n_iter=100,\n",
    "                                         scoring='neg_root_mean_squared_error', cv=5, n_jobs=-1, random_state=42)\n",
    "\n",
    "lasso_random_search = RandomizedSearchCV(lasso_pipeline, lasso_param_grid, n_iter=100,\n",
    "                                         scoring='neg_root_mean_squared_error', cv=5, n_jobs=-1, random_state=42)\n",
    "\n",
    "# Fit the best models\n",
    "svr_random_search.fit(X_train, y_train)\n",
    "svr_best = svr_random_search.best_estimator_\n",
    "\n",
    "lgbm_random_search.fit(X_train, y_train)\n",
    "lgbm_best = lgbm_random_search.best_estimator_\n",
    "\n",
    "xgb_random_search.fit(X_train, y_train)\n",
    "xgb_best = xgb_random_search.best_estimator_\n",
    "\n",
    "ridge_random_search.fit(X_train, y_train)\n",
    "ridge_best = ridge_random_search.best_estimator_\n",
    "\n",
    "lasso_random_search.fit(X_train, y_train)\n",
    "lasso_best = lasso_random_search.best_estimator_\n",
    "\n",
    "# Ensemble models\n",
    "voting_ensemble = VotingRegressor([('svr', svr_best), ('lgbm', lgbm_best), ('xgb', xgb_best),\n",
    "                                   ('ridge', ridge_best), ('lasso', lasso_best)])\n",
    "\n",
    "stacking_ensemble = StackingRegressor([('svr', svr_best), ('lgbm', lgbm_best), ('xgb', xgb_best),\n",
    "                                       ('ridge', ridge_best), ('lasso', lasso_best)],\n",
    "                                      final_estimator=Ridge())\n",
    "\n",
    "# Fit the ensembles\n",
    "voting_ensemble.fit(X_train, y_train)\n",
    "stacking_ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "voting_predictions = voting_ensemble.predict(X_test)\n",
    "stacking_predictions = stacking_ensemble.predict(X_test)\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'id': ids_test,\n",
    "    'pm2_5': voting_predictions\n",
    "})\n",
    "predictions_df.to_csv('test_predictions_ensemble_optimized_claude_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d60b17-a8fe-4c8b-a49a-ae0a651fbc4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4106487a-31c6-4c89-acab-ed4b8ccd13c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
